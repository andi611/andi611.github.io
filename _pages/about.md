---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

# Table of Content
- [About Me](#about-me)
- [Work Experience](#work-experience)
- [Extracurricular Experience](#extracurricular-experience)
- [Honors](#honors)
- [Presentations](#presentations)
- [Publications](#publications)
- [Professional Volunteer Experience](#professional-volunteer-experience)
- [Teaching Assistant Experience](#teaching-assistant-experience)
- [Interests](#interests)

# About Me
My name is Andy T. (Ting-Wei) Liu.
I received my Ph.D. in [Electrical Engineering & Computer Science (EECS)](https://www.ntu.edu.tw/english/academics/academics_electrical.html) from the [National Taiwan University (NTU)](https://www.ntu.edu.tw/english/) in January 2024, under the supervision of Prof. [Hung-yi Lee](https://speech.ee.ntu.edu.tw/~hylee/). As part of the "Speech Processing and Machine Learning" lab at NTU, I also had the privilege of collaborating with Prof. [Lin-shan Lee](http://speech.ee.ntu.edu.tw/previous_version/lslNew.htm).

With over five years of dedicated experience, I specialize in **Self-Supervised Learning (SSL), Speech Foundation Models, Automatic Speech Recognition (ASR), Large Language Models (LLM), and Multimodal Models**. My research has garnered significant attention, accumulating [**over 1,800 citations and an h-index of 12 on Google Scholar**](https://scholar.google.com/citations?user=3FpZleMAAAAJ&hl=en).

One of my publications, "TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech," was recognized by the IEEE Signal Processing Society as one of the [**top 25 downloaded articles**](https://ieeexplore.ieee.org/document/9478264) from Sept. 2021 - Sept. 2022 for IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP) on IEEE Xplore®.

As a co-founder of the [S3PRL Toolkit](https://github.com/s3prl/s3prl), which has earned over 2,100 stars on GitHub, I've been actively developing this resource since 2019. The S3PRL Toolkit provides an easy-to-use interface for the research community, offering access to self-supervised foundation models and downstream tasks.

I am passionate about advancing speech processing and machine learning technologies, and I remain committed to bridging the gap between cutting-edge research and practical applications in this dynamic field.

[Back](#table-of-content)

# Work Experience
- **Doctoral Research Assistant**<br/>
    [National Taiwan University (NTU)](https://www.ntu.edu.tw/english/)<br/>
    Taipei, Taiwan, Sep. 2018 - Jan. 2024.<br/>
    - When pursuing my PhD, I was advised by Prof. [Hung-yi Lee](https://speech.ee.ntu.edu.tw/~hylee/) and Prof. [Lin-shan Lee](http://speech.ee.ntu.edu.tw/previous_version/lslNew.htm) from the “Speech Processing and Machine Learning” lab at [NTU](https://www.ntu.edu.tw/english/).

- **Applied Scientist Intern**<br/>
    [Amazon Web Services (AWS)](https://aws.amazon.com/)<br/>
    New York, U.S.A, Jun. 2021 - Oct. 2021.<br/>
    - Worked with the Amazon Transcribe team under the supervision of [Andrew Arnold](https://www.linkedin.com/in/andrewoarnold/) and [Wei Xiao](https://www.linkedin.com/in/wei-xiao-0b973361/).<br/>
    - Completed a research project that uses LLM (Large Language Models) to achieve low-resource NER (Named Entity Recognition).<br/>

- **Ph.D. Researcher**<br/>
    [ASUS Intelligent Cloud Services (AICS)](https://aics.asus.com/)<br/>
    Taipei, Taiwan, Sep. 2020 - Apr. 2024.<br/>
    - Under the AICS Ph.D. Student Program.
    - Worked with the AI research team in Singapore remotely under the supervision of [Stefan Winkler](https://www.linkedin.com/in/winklers/) (Nov. 2021 - Jan 2024).<br/>
    - Worked with the AI medical team under the supervision of Professor [Chih-Jen Lin](https://www.csie.ntu.edu.tw/~cjlin/) and Professor [Victor Tsai](https://www.cs.nccu.edu.tw/~mftsai/).<br/>

[Back](#table-of-content)

# Extracurricular Experience

- The S3PRL Toolkit: Self-Supervised Speech Pre-training and Representation Learning [ [repo](https://github.com/s3prl/s3prl) ![GitHub stars](https://img.shields.io/github/stars/s3prl/s3prl?style=social&label=Star&maxAge=2592000) ]
- SUPERB: Speech processing Universal PERformance Benchmark [ [website](https://superbbenchmark.org/) ![GitHub stars](https://img.shields.io/github/stars/s3prl/s3prl?style=social&label=Star&maxAge=2592000) ]
- ZeroSpeech TTS-without-T Challenge [ [repo](https://github.com/andi611/ZeroSpeech-TTS-without-T) ]
- Tacotron English TTS [ [repo](https://github.com/andi611/TTS-Tacotron-Pytorch) ]
- Tacotron Code-Switch TTS [ [repo](https://github.com/andi611/CS-Tacotron-Pytorch) ]
- Sequence GAN Chatbot [ [repo](https://github.com/andi611/Conditional-SeqGAN-Tensorflow) ]

[Back](#table-of-content)

# Honors
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">ASUS Ph.D. Scholarship (華碩AI研發中心博士生學位計畫)</span> <span style="flex:  0 0 auto"><i>Sep. 2020 - Jun. 2023</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Merry Electro-Acoustic Thesis Award - 1st Place (美律電聲論文獎 - 金質獎)</span> <span style="flex:  0 0 auto"><i>2020</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">NTU Frontier Speech Technology Scholarship (國立臺灣大學前瞻語音科技獎學金)</span> <span style="flex:  0 0 auto"><i>Oct. 2019 - Aug. 2020</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">FAOS Outstanding Students Conference Travel Grant (傑出人才優秀學生出國開會補助)</span> <span style="flex:  0 0 auto"><i>2019</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">NTU Electrical Engineering Innovation Award - 2nd place (國立臺灣大學電機系精專獎 - 貳獎)</span> <span style="flex:  0 0 auto"><i>2017</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Macau Government Lotus Award (澳門政府蓮花獎)</span> <span style="flex:  0 0 auto"><i>2014</i></span></p>

[Back](#table-of-content)

# Presentations
- **Lecture in MLSS 2021 TAIPEI (Machine Learning Summer School)**, presenter of [The S3PRL Toolkit Tutorial](https://youtu.be/PkMFnS6cjAc): Self-Supervised Speech Pre-training and Representation Learning, *Taipei, Taiwan, Aug 2021*
- **Lecture in Department of Electrical Engineering at NTU**, presenter of the lecture [Audio BERT](https://youtu.be/NN9Q9Jhtvvg) in the Deep Learning for Human Language Processing course, *Taipei, Taiwan, June 2020*
- **Talk at ASUSTeK Computer Inc.**, presenter of the talk [Self-Supervised Learning for Speech](files/20200514_asus_aics_SSL_for_speech.pdf), *Taipei, Taiwan, May 2020*
- **ICASSP 2020 Lecture Session**, presenter of the paper [Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders](files/20200507_icassp_Mockingjay.pdf), *Virtual, Online, May 2020*
- **Talk at National Taiwan University**, presenter of the paper [Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders](files/20200207_NTU_foreign_guest.pdf), *NTU, Taipei, Taiwan, February 2020*
- **INTERSPEECH 2019 Oral Session**, presenter of the paper [Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion](files/20190917_interspeech_zerospeech.pdf), *Graz, Austria, September 2019*

[Back](#table-of-content)

# Publications
*Sorted by recency
- **On the social bias of speech self-supervised models**<br/>
    Yi-Cheng Lin, Tzu-Quan Lin, Hsi-Che Lin, <u>Andy T. Liu</u>, Hung-yi Lee<br/>
    *Accepted by INTERSPEECH 2024, conference organized by the International Speech Communication Association (ISCA)*<br/>
    [ [isca](https://isca-speech.org/event-5458779) | [arxiv](https://arxiv.org/abs/2406.04997) | [pdf](https://arxiv.org/pdf/2406.04997)

- **A Large-Scale Evaluation of Speech Foundation Models**<br/>
    Shu-wen Yang, <u>Andy T. Liu</u>, Heng-Jui Chang, Zili Huang, Cheng-I Lai, Haibin Wu, Jiatong Shi, Xuankai Chang, Hsiang-Sheng Tsai, Wen-Chin Huang, Tzu-hsun Feng, Po-Han Chi, Yist Y. Lin, Yung-Sung Chuang, Tzu-Hsien Huang, Wei-Cheng Tseng, Kushal Lakhotia, Shang-Wen Li, Abdelrahman Mohamed, Shinji Watanabe, Hung-yi Lee<br/>
    *Published in IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 32, 2024 (TASLP)*<br/>
    [ [ieee](https://ieeexplore.ieee.org/document/10502279) | [arxiv](https://arxiv.org/abs/2404.09385) ]

- **Parallel Synthesis for Autoregressive Speech Generation**<br/>
    Po-chun Hsu, Da-rong Liu, <u>Andy T. Liu</u>, Hung-yi Lee<br/>
    *Published in IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 31, 2023 (TASLP)*<br/>
    [ [ieee](https://ieeexplore.ieee.org/abstract/document/10209552) | [arxiv](https://arxiv.org/abs/2204.11806) ]

- **QaNER: Prompting question answering models for few-shot named entity recognition**<br/>
    <u>Andy T. Liu</u>, Wei Xiao, Henghui Zhu, Dejiao Zhang, Shang-Wen Li, Andrew Arnold<br/>
    *Work done while interning at Amazon AI. arXiv preprint, 2022, Cornell University*<br/>
    [ [arxiv](https://arxiv.org/abs/2203.01543) | [pdf](https://arxiv.org/pdf/2203.01543.pdf) ]
 
- **SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities**<br/>
    Hsiang-Sheng Tsai, Heng-Jui Chang, Wen-Chin Huang, Zili Huang, Kushal Lakhotia, Shu-wen Yang, Shuyan Dong, <u>Andy T. Liu</u>, Cheng-I Jeff Lai, Jiatong Shi, Xuankai Chang, Phil Hall, Hsuan-Jui Chen, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, Hung-yi Lee<br/>
    *Accepted by ACL 2022, conference organized by the Association for Computational Linguistics Association (ISCA)*<br/>
    [ [acl](https://aclanthology.org/2022.acl-long.580/) | [arxiv](https://arxiv.org/abs/2203.06849) | [pdf](https://aclanthology.org/2022.acl-long.580.pdf) | [code](https://github.com/s3prl/s3prl) ]
    
- **SUPERB: Speech processing Universal PERformance Benchmark**<br/>
    Shu-wen Yang, <u>Andy T Liu</u>, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y Lin, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko-tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, Hung-yi Lee<br/>
    *Accepted by INTERSPEECH 2021, conference organized by the International Speech Communication Association (ISCA)*<br/>
    [ [isca](https://www.isca-speech.org/archive/interspeech_2021/yang21c_interspeech.html) | [arxiv](https://arxiv.org/abs/2105.01051) | [pdf](https://www.isca-archive.org/interspeech_2021/yang21c_interspeech.pdf) | [code](https://github.com/s3prl/s3prl) ]

- **Improving the Adversarial Robustness for Speaker Verification by Self-Supervised Learning**<br/>
    Haibin Wu, Xu Li, <u>Andy T. Liu</u>, Zhiyong Wu, Helen Meng, Hung-yi Lee<br/>
    *Published in IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 30, 2021 (TASLP)*<br/>
    [ [ieee](https://ieeexplore.ieee.org/document/9645217) | [arxiv](https://arxiv.org/abs/2106.00273) | [pdf](https://arxiv.org/pdf/2106.00273.pdf) | [code](https://github.com/s3prl/s3prl) ]

- **Don't Speak Too Fast: The Impact of Data Bias on Self-Supervised Speech Models**<br/>
    Yen Meng, Yi-Hui Chou, <u>Andy T. Liu</u>, Hung-yi Lee<br/>
    *Lecture session in ICASSP 2022, conference organized by the IEEE Signal Processing Society (SPS)*<br/>
    [ [ieee](https://ieeexplore.ieee.org/abstract/document/9747897) | [arxiv](https://arxiv.org/abs/2110.07957) | [pdf](https://arxiv.org/pdf/2110.07957.pdf) | [code](https://github.com/s3prl/s3prl) ]

- **TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech**<br/>
    <u>Andy T. Liu</u>, Shang-Wen Li, Hung-yi Lee<br/>
    *Published in IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021 (TASLP)*<br/>
    [ [ieee](https://ieeexplore.ieee.org/document/9478264) | [arxiv](https://arxiv.org/abs/2007.06028) | [pdf](https://bit.ly/ieee-tera-pdf) | [code](https://github.com/s3prl/s3prl) ]

- **Adversarial Defense for Automatic Speaker Verification by Cascaded Self-Supervised Learning Models**<br/>
    Haibin Wu, Xu Li, <u>Andy T. Liu</u>, Zhiyong Wu, Helen Meng, Hung-yi Lee<br/>
    *Virtual session in ICASSP 2021, a conference organized by the IEEE Signal Processing Society (SPS)*<br/>
    [ [ieee](https://ieeexplore.ieee.org/document/9413737) | [arxiv](https://arxiv.org/abs/2102.07047) | [pdf](https://arxiv.org/pdf/2102.07047.pdf) | [code](https://github.com/s3prl/s3prl) ]

- **Defense for Black-box Attacks on Anti-spoofing Models by Self-Supervised Learning**<br/>
    Haibin Wu, <u>Andy T. Liu</u>, Hung-yi Lee<br/>
    *Virtual session in INTERSPEECH 2020, a conference organized by the International Speech Communication Association (ISCA)*<br/>
    [ [isca](https://www.isca-speech.org/archive/interspeech_2020/wu20m_interspeech.html) | [arxiv](https://arxiv.org/abs/2006.03214) | [pdf](https://www.isca-archive.org/interspeech_2020/wu20m_interspeech.pdf) | [code](https://github.com/s3prl/s3prl) ]

- **Understanding Self-Attention of Self-Supervised Audio Transformers**<br/>
    Shu-wen Yang, <u>Andy T. Liu</u>, Hung-yi Lee<br/>
    *Virtual session in INTERSPEECH 2020, a conference organized by the International Speech Communication Association (ISCA)*<br/>
    [ [isca](https://www.isca-speech.org/archive/interspeech_2020/yang20i_interspeech.html) | [arxiv](https://arxiv.org/abs/2006.03265) | [pdf](https://www.isca-archive.org/interspeech_2020/yang20i_interspeech.pdf) | [demo](https://github.com/leo19941227/Self-Attention-on-SATs) ]

- **Towards Robust Neural Vocoding for Speech Generation: A Survey**<br/>
    Po-chun Hsu, Chun-hsuan Wang, <u>Andy T. Liu</u>, Hung-yi Lee<br/>
    *arXiv preprint, 2020, Cornell University*<br/>
    [ [arxiv](https://arxiv.org/abs/1912.02461) | [pdf](https://arxiv.org/pdf/1912.02461) | [demo](https://bogihsu.github.io/Robust-Neural-Vocoding/) ]

- **Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders**<br/>
    <u>Andy T. Liu</u>, Shu-wen Yang, Po-Han Chi, Po-chun Hsu, Hung-yi Lee<br/>
    *Lecture session in ICASSP 2020, a conference organized by the IEEE Signal Processing Society (SPS)*<br/>
    [ [ieee](https://ieeexplore.ieee.org/document/9054458) | [arxiv](https://arxiv.org/abs/1910.12638) | [pdf](https://bit.ly/ieee-mockingjay-pdf) | [code](https://github.com/andi611/Mockingjay-Speech-Representation) | [slide](https://bit.ly/icassp2020-mockingjay) | [talk](https://youtu.be/THylmb3hZVs) ]

- **Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion**<br/>
    <u>Andy T. Liu</u>, Po-chun Hsu, Hung-yi Lee<br/>
    *Oral session in INTERSPEECH 2019, conference organized by the International Speech Communication Association (ISCA)*<br/>
    [ [isca](https://www.isca-speech.org/archive/interspeech_2019/liu19c_interspeech.html) | [arxiv](https://arxiv.org/abs/1905.11563) | [pdf](https://www.isca-archive.org/interspeech_2019/liu19c_interspeech.pdf) | [code](https://github.com/andi611/ZeroSpeech-TTS-without-T) | [slide](http://bit.ly/20190917_interspeech_talk) ]

[Back](#table-of-content)

# Professional Volunteer Experience
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Official reviewer of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6570655">IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)</a></span> <span style="flex:  0 0 auto"><i>Journal, 2024</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Official reviewer of <a href="https://2024.aclweb.org/">ACL 2024</a></span> <span style="flex:  0 0 auto"><i>Conference, 2024</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Official reviewer of <a href="https://interspeech2024.org/">INTERSPEECH 2024</a></span> <span style="flex:  0 0 auto"><i>Conference, 2024</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Official reviewer of <a href="https://2024.ieeeicassp.org/">ICASSP 2024</a></span> <span style="flex:  0 0 auto"><i>Conference, 2024</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Official reviewer of <a href="https://2023.ieeeicassp.org/">ICASSP 2023</a></span> <span style="flex:  0 0 auto"><i>Conference, 2023</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Official reviewer of <a href="https://signalprocessingsociety.org/publications-resources/ieee-open-journal-signal-processing">IEEE Open Journal of Signal Processing (OJSP)</a></span> <span style="flex:  0 0 auto"><i>Journal, 2023</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Official reviewer of <a href="https://signalprocessingsociety.org/publications-resources/ieee-open-journal-signal-processing">IEEE Open Journal of Signal Processing (OJSP)</a></span> <span style="flex:  0 0 auto"><i>Journal, 2022</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Official reviewer of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=4200690">IEEE Journal of Selected Topics in Signal Processing (JSTSP)</a></span> <span style="flex:  0 0 auto"><i>Journal, 2022</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Official reviewer of <a href="https://2022.ieeeicassp.org/">ICASSP 2022</a></span> <span style="flex:  0 0 auto"><i>Conference, 2022</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Official reviewer of <a href="http://www.iscslp2022.org/">ISCSLP 2022</a></span> <span style="flex:  0 0 auto"><i>Conference, 2022</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Official reviewer of <a href="https://2021.emnlp.org/">EMNLP 2021</a></span> <span style="flex:  0 0 auto"><i>Conference, 2021</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Official reviewer of <a href="https://neurips-sas-2020.github.io/">NeurIPS Workshop: Self-Supervised Learning for Speech and Audio Processing</a></span> <span style="flex:  0 0 auto"><i>Workshop, 2020</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Review Helper of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6570655">IEEE/ACM TASLP</a></span> <span style="flex:  0 0 auto"><i>Journal, 2020</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Review Helper of <a href="http://www.iscslp2021.org/">ISCSLP 2020</a></span> <span style="flex:  0 0 auto"><i>Conference, 2020</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Review Helper of <a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a></span> <span style="flex:  0 0 auto"><i>Conference, 2020</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Review Helper of <a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a></span> <span style="flex:  0 0 auto"><i>Conference, 2019</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Review Helper of <a href="https://2020.ieeeicassp.org/">ICASSP 2020</a></span> <span style="flex:  0 0 auto"><i>Conference, 2019</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Review Helper of <a href="https://interspeech2019.org/">INTERSPEECH 2019</a></span> <span style="flex:  0 0 auto"><i>Conference, 2019</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">Review Helper of <a href="http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=87194&copyownerid=13881">ACMSE 2019</a></span> <span style="flex:  0 0 auto"><i>Conference, 2019</i></span></p>

[Back](#table-of-content)

# Teaching Assistant Experience
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">TA of <a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses.html">Machine Learning Special Project</a></span> <span style="flex:  0 0 auto"><i>NTU EECS, 2018-Pres.</i></span></p>
- <p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;"><span style="flex: 0 0 auto">TA of <a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html">Machine Learning and Having it Deep and Structured</a></span> <span style="flex:  0 0 auto"><i>NTU EECS, Fall 2019</i></span></p>

[Back](#table-of-content)

# Interests
- [PADI](https://www.padi.com/courses/open-water-diver?lang=en) Scuba Diving Instructor, May. 2023 - Present
- Scuba Diving, Jul. 2020 - Present
- Photographer, Amateur, Nov. 2021 - Present
- Guitar, Amateur, Oct. 2016 - Jan. 2020
- Road Biking, ([NTU Cycling Club](https://www.facebook.com/ntucyc)), Jun. 2014 - Aug. 2017
- Pet geckos: [Instagram](https://www.instagram.com/smiling._.dragons/)

[Back](#table-of-content)

# Contact
[liuandyt@gmail.com](mailto:liuandyt@gmail.com)

Write me if you are looking for a collaboration!
